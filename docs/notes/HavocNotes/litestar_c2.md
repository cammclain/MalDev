Building a Stealthy Litestar-Based C2: Best Practices
1. Zig Implant Builder as a Dedicated Service
Isolating the implant builder into its own Litestar app (deployed via Docker Compose) brings modularity and security. Modern C2 frameworks like Mythic embrace a microservice architecture where components such as payload generators run in separate containers for maintainability and isolation​
POSTS.SPECTEROPS.IO
​
DOCS.MYTHIC-C2.NET
. By decoupling the Zig-based implant compiler into its own service, the main C2 server remains lightweight and is shielded from the risks of running a compiler on user input.API-Driven Build Process: Expose an HTTP API for build requests so operators can submit implant configurations (e.g. callback address, encryption keys) and receive a compiled binary. This service should take the config, template it into Zig source or use Zig’s build options, then invoke the Zig compiler. Zig is well-suited here because it simplifies cross-compilation for multiple platforms – the Zig toolchain can target Windows, Linux, and macOS from a single environment with minimal fuss​
PIEMBSYSTECH.COM
​
BLOG.Z-LABS.EU
. In practice, “with Zig, you don’t need to manually install or configure multiple compilers… the Zig compiler handles everything automatically,” enabling one-step cross-builds for different OS and architectures​
PIEMBSYSTECH.COM
. This means your builder container (with Zig installed) can produce Windows DLLs, Linux ELF binaries, etc., all on demand.Secure Subprocess Execution: Building implants involves running a compiler, which should be handled carefully to avoid compromising the C2. If using subprocess calls (e.g. zig build), run them with least privilege. Docker confinement is key – ensure the builder container runs as a non-root user and has no excess permissions. As Docker’s own docs note, by default container processes might have more privileges than desired; “Docker recommends… running your processes inside containers as non-privileged users (i.e., non-root)”​
SECURITY.STACKEXCHANGE.COM
. This limits impact if the Zig compiler or build script is exploited. Additionally, avoid directly concatenating user input into shell commands (to prevent injection). If the configuration is purely data (not arbitrary code), sanitize and validate it before use. In a more advanced setup, you could run the compiler in a sandbox (using Linux seccomp or even a lightweight VM) for defense-in-depth​
SECURITY.STACKEXCHANGE.COM
​
SECURITY.STACKEXCHANGE.COM
, but a dedicated container is usually sufficient for an internal build service.Deployment Considerations: The builder service can be scaled or replaced independently. For example, if many build requests are expected, you might run multiple instances behind an internal API gateway. Zig’s cross-compilation efficiency also reduces the need for separate builders per OS – one service can handle multi-platform output​
PIEMBSYSTECH.COM
. Ensure the container has the necessary Zig standard library and any Zig “zig-cache” warmed up, to speed up builds. Finally, store or cache compiled artifacts securely. You might allow the API to return the binary directly or provide a one-time download link. Treat these binaries as sensitive – implement access controls on the build API (authentication or internal network access only) so that only authorized operators trigger builds. By following these practices, the builder service remains a rapid yet safe pipeline for producing implants, without bogging down or risking the core C2 application​
DOCS.MYTHIC-C2.NET
.
2. Secure Communications Between Cloudflare Workers and the Onion Proxy
Using Cloudflare Workers as an external face for your C2 (with the actual C2 hidden behind Tor) can greatly enhance stealth by blending C2 traffic with normal web traffic. However, it introduces a potential man-in-the-middle at the edge (Cloudflare), so careful design is needed to preserve confidentiality and integrity. Cloudflare Workers are essentially serverless code running on Cloudflare’s edge, sitting transparently between the client (implant) and the origin server​
BLOG.CHRISTOPHETD.FR
 (in our case, the origin is the Tor .onion service). The Worker will receive implants’ HTTP(S) requests and then fetch and forward data to/from the hidden service.

Cloudflare Workers sit between implants and your hidden service, proxying requests. Extra encryption and careful routing can prevent the Cloudflare layer from exposing or tampering with C2 data​
BLOG.CHRISTOPHETD.FR
.End-to-End Encryption & Integrity: Do not rely solely on the TLS that the implant uses to connect to Cloudflare. Since the Worker will decrypt that traffic to relay it, Cloudflare’s infrastructure could see plaintext C2 commands/responses. To mitigate this, add an application-layer encryption to your C2 protocol. In practice, the implant and C2 server should share cryptographic keys to encrypt all payloads before they ever hit the network. That way, the Cloudflare Worker is relaying opaque ciphertext. Even if Cloudflare inspected the traffic, it would see nothing useful, and any tampering would break decryption or message signatures. This is a common design in modern C2s – for example, Cobalt Strike beacons encrypt task data with a session key, so any intermediary (CDN, proxy) only forwards gibberish. The importance of this is highlighted by prior research on Tor proxies: when using an HTTP proxy to reach an .onion service, the proxy could log or alter data if it’s unencrypted​
MDSEC.CO.UK
. In our case, double-encryption (TLS to Cloudflare plus inner encryption to the C2) ensures data confidentiality and integrity even though Cloudflare terminates the outer TLS. In short, treat Cloudflare as an untrusted hop and encrypt everything end-to-end between implant and C2 server.Preserving Anonymity & Metadata Hygiene: One big goal is to prevent anyone from discovering your actual C2 onion address or other metadata. When the implant connects to the Cloudflare Worker (via a Cloudflare-provided hostname), it should use HTTPS and a covert domain that doesn’t raise suspicion. The Worker’s code will then fetch the hidden service. Ensure that the onion address isn’t embedded in any client-visible responses or DNS queries. Unlike using a public Tor gateway (like onion.cab), where the target onion might leak via the HTTP Host header or proxy logs​
MDSEC.CO.UK
, your Cloudflare Worker can keep the onion address purely server-side. The implant’s HTTP request goes to a benign-looking domain (Cloudflare edge); only the Worker knows the .onion destination. This prevents enterprise proxies from detecting the onion name – even if a corporate firewall intercepts TLS (with their own certificate), it would see an HTTPS request to your front domain, not the .onion. The Cloudflare Worker should also strip any identifying headers. Do not forward the implant’s true User-Agent or other fingerprintable info to the onion service unless needed, to avoid correlation. Keep in mind Cloudflare’s network itself will see an outgoing connection from the Worker to Tor. If possible, use Cloudflare’s Onion Service feature or have the Worker connect through Tor. Cloudflare’s Onion Service essentially allows Cloudflare to act as a Tor client in their infrastructure, removing the normal Tor exit node from the equation​
BLOG.CLOUDFLARE.COM
. This means Cloudflare would establish the Tor circuit directly to your hidden service, improving security. If that’s not available, the Worker might have to call an external Tor gateway you control (introducing another hop). In any case, prefer end-to-end Tor encryption for that leg. The Tor hidden service protocol itself encrypts traffic within the Tor circuit​
BLOG.CLOUDFLARE.COM
, so even Cloudflare only sees random Tor circuit data on the link to the onion.Mitigating Cloudflare MITM Risks: We inherently trust the Cloudflare Worker to forward traffic correctly, but to reduce risk, keep the Worker code minimal – ideally just a simple fetch to the onion and return the response. Avoid complex logic that could accidentally leak info. Also, do not log sensitive data in the Worker (Cloudflare allows console logging – don’t print keys or contents). Rely on the encryption such that even if Cloudflare inspected memory or was coerced by legal request, they see only encrypted blobs. For integrity, you could sign tasks so that implants can verify commands originate from the legitimate C2 (and not a tampered Worker). Essentially, treat the Cloudflare path as a potentially untrusted proxy – much like how onion.cab “could be logging traffic and would be able to advise what hidden service [the client] is connecting to” if not protected​
MDSEC.CO.UK
. By never exposing the onion address or plaintext data at the edge, you greatly reduce this risk.Avoiding C2 Exposure: The hidden service should only be reachable via intended channels (Cloudflare Worker or other approved proxies). Do not accidentally expose it on clearnet. For example, double-check that your Litestar app isn’t listening on a public interface – bind it to localhost or a Tor network interface only. Also avoid reuse of any certificates or unique identifiers between the onion service and other servers​
SECJUICE.COM
. A famous pitfall is using the same TLS certificate on the onion and a public site – researchers have shown that scanning for a certificate’s fingerprint on the internet can reveal the real IP of the host​
SECJUICE.COM
​
SECJUICE.COM
. If you do use TLS on the onion (optional, since Tor is already encrypted), use a self-signed or unique cert that you don’t use elsewhere. Overall, all roads should lead into Cloudflare and then to Tor – there should be no direct route to your C2 that leaks its location or identity.In summary, by fronting your C2 with Cloudflare Workers, you achieve strong traffic obfuscation (appearing as Cloudflare traffic), but you must compensate by encrypting at the application layer and strictly controlling what the Worker knows. This strategy prevents metadata leakage and makes it extremely hard for defenders to trace or decrypt your C2 traffic​
MDSEC.CO.UK
, all while leveraging Cloudflare’s reliability and global presence.
3. Listener Architecture for the Tor Hidden Service
Your actual C2 server will be a Litestar application, and the plan is to expose it only via a Tor v3 hidden service on port 8080 (no clearnet exposure). This setup provides anonymity and makes takedown or tracking much harder, but it requires careful configuration to be robust and secure.Single Listener, Onion Exposure: Running a single Litestar instance on port 8080 is fine – Tor will forward incoming onion requests to this port. Ensure Tor is configured with the correct HiddenServicePort mapping (e.g. 80 or 443 on the onion to 8080 internally). It’s wise to use Tor’s latest version and ed25519 onion addresses (v3), as they have stronger crypto and longer addresses that are practically unguessable. The hidden service’s private key should be kept secure (Tor writes it to hs_ed25519_secret_key). If this key is compromised, your onion’s security is gone.Resilience and High Availability: A drawback of a single Tor hidden service is that if the server or Tor process goes down, implants lose contact. To improve uptime, consider OnionBalance or similar techniques. OnionBalance is a tool that allows you to run multiple instances of a hidden service on different machines and load-balance them​
BLOG.TORPROJECT.ORG
​
BLOG.TORPROJECT.ORG
. Essentially, you give OnionBalance the keys or descriptors of each backend, and it publishes a “super-descriptor” for one onion address containing multiple introduction points. Clients (implants) connecting to the onion will be distributed to one of the backend servers, providing both load balancing and redundancy​
BLOG.TORPROJECT.ORG
. This is analogous to DNS round-robin for regular sites​
BLOG.TORPROJECT.ORG
. If one backend goes offline, the others can continue to serve. For our C2, you could deploy two or more Litestar listeners (each with their own Tor instance and onion key copy) and use OnionBalance so the .onion address remains the same. This way, taking down one server doesn’t knock the C2 completely offline, and you can also handle more load (if you ever need to scale to many implants concurrently). At minimum, backup your onion keys so you can restore the service quickly on a new host if needed.Another resiliency strategy is to mix proxy layers. You already have Cloudflare Workers as one layer; you could add others as fallback. For example, set up a secondary front: perhaps another CDN or a different domain pointing to a different Cloudflare Worker, or use a platform like Azure Functions or Google Cloud Run to proxy to the onion. The idea is to have multiple egress points for the implant to reach the onion. If one front domain is blocked or Cloudflare intervenes, the implant could try the backup. Some C2 designs even rotate through several domain fronts. Just ensure all fronts ultimately connect to the same hidden service and enforce the same encryption scheme. This multi-layer approach increases resiliency against blocking. Important: keep the onion service itself stable – long-lived onion addresses that suddenly disappear and reappear raise suspicion. Run Tor and your Litestar app under something like systemd or a supervisor so they restart on crash. Monitoring is also useful: have a script that checks the onion’s responsiveness periodically.Onion Service Security: Because the C2 is only accessible via Tor, typical network attacks are reduced (no scanner is hitting your IP). But you should still harden the Litestar app – use authentication for the control endpoints, and ideally require implants to authenticate as well (maybe via an implant ID and shared secret on each check-in). Tor also offers an additional layer called Client Authorization, where the hidden service can be configured to require an auth token from clients to even connect​
TPO.PAGES.TORPROJECT.NET
. You generate a client auth key and give it to your implants (Tor Browser or the Tor client can use it). Only those with the key can access the onion service; others get no response. This can prevent anyone else from even probing your C2 if they somehow discovered the onion address. It’s worth considering for very stealthy ops, though managing distribution of the auth tokens to all implants can be complex. Another best practice is isolating the hidden service host from the internet entirely. Many recommend hosting a hidden service using the Whonix architecture – i.e. one VM for the server, and a separate Tor gateway VM for networking​
REDDIT.COM
. This way even if the server is compromised, it has no real IP to leak (it only talks to the Tor gateway). As one user noted, “using Whonix… any software bugs that may reveal the server IP won’t actually reveal anything thanks to the gateway”, preventing a common failure mode that de-anonymizes services​
REDDIT.COM
. If a full Whonix VM setup is too heavy, ensure the server at least routes all traffic through Tor (and has no DNS or clearnet routes). Do not install unnecessary services on the host that might beacon out and give away its location. Also, be mindful of timing correlations – e.g., if you access the host via SSH for admin, do that through Tor as well, or out-of-band, to avoid correlating an SSH session from an IP with activity on the onion.Preventing Discovery: Aside from network hardening, operational security matters. Keep the onion address secret – don’t use it anywhere except in your implants and needed infrastructure. If you use automated deployment, avoid logging the onion URL in cloud logs or tickets. The randomness of v3 addresses gives you protection (the address is essentially a 56-character public key). Also, when building implants, embed the onion in a way that’s not trivial to extract (to slow down reverse engineering, though a motivated analyst will eventually find it). Changing the onion address periodically is an option (forcing implants to have a list of addresses or a mechanism to get new addresses), but this can be risky if an implant misses the update. Many C2 operators stick with one stable onion per campaign for simplicity.Multiple Layers of Proxies: As mentioned, using multiple proxy layers (Cloudflare, other CDNs, or even domain fronting if possible) can make your C2 communication more robust. Domain fronting, historically used to disguise traffic by sending it to one host but indicating another in the TLS SNI/HTTP Host, has been largely neutered by most providers. However, some CDN or cloud providers might still allow semi-fronting techniques. In your case, the Cloudflare Worker is essentially performing a similar role (fronting the onion). You could register additional Cloudflare accounts or use other edge networks to do the same, providing alternative domains that implants can cycle through. Redundancy is key for a long-term stealth C2.In summary, running your Litestar C2 behind a Tor hidden service gives you a hidden IP and encrypted, anonymized channel by default. Follow Tor’s best practices: use v3 onions, consider client auth and OnionBalance for HA​
TPO.PAGES.TORPROJECT.NET
, and isolate the service host to prevent any IP leakage​
REDDIT.COM
. Combine that with layered proxies (like the Cloudflare Worker) and you obtain a resilient, truly covert C2 channel that’s hard to discover or disrupt.
4. Task Queue Implementation with a Simple Async Task Queue (SAQ)
Managing tasking and jobs in the C2 is crucial – you need a system for scheduling commands, handling responses, and cleaning up completed tasks. Simple Async Queue (SAQ) is a great fit for this in a Litestar-based C2 due to its lightweight, async nature. In fact, the Litestar reference fullstack app itself integrates SAQ for background tasks​
DOCS.FULLSTACK.LITESTAR.DEV
, highlighting it as a best practice in the Litestar ecosystem.Why SAQ (Simple Async Queue): SAQ is a fast, minimal job queue framework built on Python asyncio and Redis/Postgres​
SAQ-PY.READTHEDOCS.IO
. Unlike heavier task queues (Celery, RQ, etc.), SAQ is designed to be simple and to leverage async IO for performance​
SAQ-PY.READTHEDOCS.IO
. This means your C2 can queue and process tasks without spawning a ton of threads or processes for each job – it uses async await to handle many jobs efficiently in a few worker processes. SAQ’s own benchmarks show significantly lower overhead for async jobs​
SAQ-PY.READTHEDOCS.IO
. This is ideal for a C2, which might need to handle many small tasks (like “run this command”) and some larger jobs (exfiltrating a file or running a longer script) concurrently. By using SAQ, your Litestar server can hand off a task to the queue and immediately free itself to handle other connections (other implants). A separate SAQ worker (or pool of workers) will execute the task in the background. This keeps the main event loop snappy and prevents one slow operation from blocking others.Scalability & Simplicity: SAQ hits a sweet spot between scalability and simplicity. It uses Redis (or Postgres) as a broker/persistence layer​
SAQ-PY.READTHEDOCS.IO
, which is easy to run via Docker and doesn’t add significant complexity. With Redis as the queue backend, you can run multiple SAQ worker processes, even across multiple hosts, and they’ll all fetch tasks from the shared queue. Scaling up is as simple as running more workers (e.g., litestar ... workers run --workers 4 to start 4 workers)​
GITHUB.COM
. Because it’s so lightweight, you can embed it in the same Docker Compose as the rest of your C2 without a huge resource hit. If your C2 expands, SAQ can handle increased load by horizontally scaling workers, all while using the same simple API to enqueue tasks. This design ensures your architecture stays straightforward – you’re not introducing a complex distributed system, just a small queue component. The Litestar SAQ plugin makes integration trivial: you configure the DSN (Redis URL) and queue names, attach the plugin to the Litestar app, and you have an instant background task system​
GITHUB.COM
​
GITHUB.COM
.Managing C2 Tasks with SAQ: In a C2 context, tasks include things like operator-issued commands, file download/upload jobs, periodic beacon tasks, etc. A good practice is to categorize and separate these tasks, which SAQ supports via multiple queues. For example, you might have a queue per implant or per task type. SAQ lets you define named queues and even run dedicated workers on specific queues​
GITHUB.COM
. For instance, a “file-transfers” queue could be processed by a worker with more resources (or that runs only one job at a time to limit bandwidth), whereas a “commands” queue could handle quick exec tasks. This prevents a large file exfiltration job from starving simpler tasks. In SAQ, enqueuing a job is as easy as calling queue.enqueue(function, *args). You can write task handler functions to, say, store an incoming file chunk or execute an OS command on an implant’s behalf.However, note that in many C2 designs, the tasks (like “run this process on target”) are actually executed on the implant, not the server. So how does SAQ help? One pattern is to use SAQ to manage the state and delivery of those tasks. For example, when an operator schedules a command for an implant, you enqueue a job in SAQ representing that task. Instead of a worker executing it, the next time the implant checks in, the server can query SAQ (using SAQ’s Python API) for pending jobs for that implant, pop it, and send it to the implant. In this model, SAQ is acting as a task buffer with persistence. This is useful because if your server restarts, the queued tasks are not lost – they live in Redis. Alternatively, you might not use SAQ for the actual implant instruction delivery (since a simple in-memory list or database could suffice for that), but rather use it for server-side tasks that arise. For instance, when an implant responds with data, you might enqueue a job to process or store that data (parse results, write to a database, etc.) without blocking the main thread. Or if you have periodic tasks (like rotating the onion service, or cleaning up old logs), those can be scheduled in SAQ with delays or intervals.Efficiency and Cleanup: Ensure that once a task is completed, it’s removed or marked done so it doesn’t clog the queue. SAQ provides a web UI to monitor jobs and their status​
GITHUB.COM
, which can be helpful to see if tasks are stuck. You can configure job TTLs or timeouts in SAQ’s settings (for example, if an implant hasn’t pulled a task in X hours, you might drop it). For command execution tasks, once the implant returns the result, you can mark the job as finished and perhaps log the output, then purge it. File transfer tasks often involve multiple steps (chunks); you might create sub-tasks for each chunk and a final task that assembles them. Using SAQ’s Redis backend, these jobs can be tracked easily and even retried if something fails.One of the benefits of SAQ is that it’s simple to maintain. The configuration is minimal and it doesn’t require managing separate message broker servers (beyond Redis). It’s also written in Python, so if you need to extend its functionality, you can. In contrast, a system like Celery might be overkill and add latency. SAQ, being async and in-process, keeps task handling fast – this is important for C2 where you want minimal delay between operator action and implant response. As the SAQ docs note, it was inspired by RQ and ARQ but is asynchronous and “considerably faster due to lower overhead”​
SAQ-PY.READTHEDOCS.IO
.Simplicity in Architecture: By using SAQ, your overall C2 design stays lean. You have your Litestar app (for HTTP/Tor communication), the Zig builder service, and SAQ for tasks, all containerized. Each piece is fairly self-contained and communicate with standard protocols (HTTP for the builder, Redis for SAQ). This modular approach means you can modify one component without heavily impacting others. For example, if you needed to swap out SAQ for a different queue, you could do so without touching the builder or the network layer. But given Litestar’s own reference uses SAQ​
DOCS.FULLSTACK.LITESTAR.DEV
, you’re aligning with a proven pattern.Operational Usage: In practice, you might implement an endpoint like /tasks/<agent_id> that the implant calls to get its next task. That endpoint’s handler could do something like job = await queue.dequeue(agent_id) (using SAQ’s API to fetch a job designated for that agent). If a job is found, it returns it (perhaps encrypted) to the implant, and marks it processing. When the implant returns a result, you complete the job. SAQ can store results or you can handle that outside. The task cleanup could simply be deleting the job from Redis once done (SAQ likely handles this by default on successful job execution by a worker, but in our pull-model case, you’d handle it). Regularly prune or archive old tasks to avoid any forensic trail – once a task is done and its result logged to your secure storage, there’s no need to keep it in the queue.In conclusion, integrating a simple async task queue provides a scalable yet straightforward way to handle C2 tasking. You gain the ability to queue up multiple actions, process long-running jobs asynchronously, and scale out to handle many implants – all without complicating the development too much. SAQ’s lightweight design and Litestar plugin support make it a natural choice to achieve this goal, keeping your C2 responsive, scalable, and easy to deploy​
SAQ-PY.READTHEDOCS.IO
​
DOCS.FULLSTACK.LITESTAR.DEV
.